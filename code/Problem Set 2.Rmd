---
title: "Problem Set 2: Bias, Variance, Cross-Validation"
author: "<CANDIDATE NUMBER HERE>"
date: |
  | `r format(Sys.time(), '%d %B %Y')`
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
options(repr.plot.width=4, repr.plot.height=3)
library(ggplot2)

plot_decision_boundary <- function(train_x, train_y, pred_grid, grid) {
  cl <- ifelse(train_y == 1, "Pos", "Neg")
  # Data structure for plotting
  
  dataf <- data.frame(grid,
                      prob = as.numeric(pred_grid), #prob = attr(pred_grid, "prob"),
                      class = ifelse(pred_grid==2, "Pos", "Neg"))
  
  ## Plot decision boundary
  
  col <- c("#009E73", "#0072B2") # Hex color codes
  plot <- ggplot(dataf) +
    geom_raster(aes(x=x_1, y=x_2, fill=prob), alpha=.9,
                 data=dataf) +
    geom_point(aes(x=x_1, y=x_2, color=class),
               size=1,
               data=data.frame(x_1=train_x[,1], x_2=train_x[,2], class=cl)) +
    geom_point(aes(x=x_1, y=x_2),
               size=1, shape=1,
               data=data.frame(x_1=train_x[,1], x_2=train_x[,2], class=cl)) + 
    scale_colour_manual(values=col, name="Class") +
    scale_fill_gradientn(colors=col[c(2,1)], limits=c(0,1), guide = FALSE) + 
    xlab("Feature 1") + ylab("Feature 2")
  return(plot)
}
```

## 1. ISLR Chapter 5 Exercise 8

a. We will now perform cross-validation on a simulated data set. In this data set, what is $n$ and what is $p$? Write out the model used to generate the data in equation form.

```{r}
set.seed(1)

x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
```

b. Create a scatterplot of $X$ against $Y$. Comment on what you find.

```{r}

```

c. Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:

- $Y = \beta_0 + \beta_1X + \varepsilon$
- $Y = \beta_0 + \beta_1X + \beta_2X^2 + \varepsilon$
- $Y = \beta_0 +\beta_1X +\beta_2X^2 +\beta_3X^3 + \varepsilon$
- $Y = \beta_0 +\beta_1X +\beta_2X^2 +\beta_3X^3 +\beta_4X^4 + \varepsilon.$

Note you may find it helpful to use the \texttt{data.frame()} function
to create a single data set containing both $X$ and $Y$.

```{r}

```

d. Repeat (c) using another random seed, and report your results.
Are your results the same as what you got in (c)? Why?

```{r}

```

e. Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.

```{r}

```

f. Comment on the statistical significance of the coefficient esti- mates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?

```{r}

```

## 2. 10-fold CV using random dataset

Below is a dataset generated by adding gaussian noise to a pre-defined function. The true function is plotted in red.
```{r}

# because we are generating random data, set a random seed
set.seed(1)

# generate values in x spread evenly from 0 to 20
x <- seq(from=0, to=20, by=0.05)

# generate y according to the following known function of x
y <- 500 + 0.4 * (x-10)^3

# add random noise to y
noise <- rnorm(length(x), mean=10, sd=80)
noisy.y <- y + noise

# plot data
# red line for true underlying function generating y
{
  plot(x,noisy.y)
  lines(x, y, col='red')
}
```

a. With predictor `x` and outcome `noisy_y`, split the data into a training and test set.

```{r}

```

b. Perform 10-fold CV for polynomials from degree 1 to 5 (use MSE as your error measure). This should be done from scratch using a for loop. *(Hint: It may be helpful to randomly permute and then split the training set from the previous section into 10 evenly sized parts. You may need an if statement to handle a potential problem in the last iteration of your loop.)*

```{r}

```

c. Plot the best model's fitted line in blue and compare to the true function (the red line from the previous plot). 

```{r}

```

d. Comment on the results of (c). Why was performance better or worse at different order polynomials?

```{r}

```

e. Report the CV error and test error at each order of polynomial. Which achieves the lowest CV error? How does the CV error compare to the test error? Comment on the results.

```{r}

```

## 3. Classifying a toy dataset

a. Pick a new dataset from the `mlbench` package (one we haven't used in class). Experiment with classifying the data using KNN at different values of k. Use cross-validation to choose your best model.

```{r}

```

b. Plot misclassification error rate at different values of k.

```{r}

```

c. Plot the decision boundary for your classifier using the function at the top code block, `plot_decision_boundary()`. Make sure you load this function into memory before trying to use it.

```{r}

```

## 4. Performance measures for classification

Recall the `Caravan` data from the week 2 lab (part of the `ISLR` package). Train a KNN model with k=2 using all the predictors in the dataset and the outcome `Purchase`. Create a confusion matrix with the test set predictions and the actual values of `Purchase`. Using the values of the confusion matrix, calculate precision, recall, and F1. (Note that `Yes` is the positive class and the confusion matrix may be differently oriented than the one presented in class.)

```{r}

```

## 5. ISLR Chapter 5 Exercise 3
